{
  "run_name": "tiny-mlx-tinystories",
  "project_name": "deepseek-from-scratch",
  "backend": "mlx",
  "model_size": "tiny",
  "model": {
    "d_model": 256,
    "num_heads": 4,
    "num_layers": 4,
    "vocab_size": 8000,
    "max_seq_len": 512,
    "d_latent": 64,
    "d_rope": 32,
    "num_experts": 4,
    "num_shared_experts": 1,
    "top_k": 1,
    "num_expert_groups": 2,
    "moe_hidden_mult": 2.0,
    "mtp_k": 2,
    "use_sparse_attention": false,
    "sparse_window_size": 4096,
    "sparse_global_tokens": 512,
    "dropout": 0.0,
    "attention_dropout": 0.0
  },
  "stages_to_run": [
    "data_prep",
    "pretrain",
    "sft",
    "grpo",
    "export"
  ],
  "data": {
    "data_dir": "./data/stories",
    "domain_paths": {},
    "cache_dir": "./cache",
    "tokenizer_name": "gpt2",
    "tokenizer_path": null,
    "domain_weights": {
      "stories": 1.0
    },
    "use_curriculum": true,
    "curriculum_start_seq_len": 128,
    "curriculum_end_seq_len": 512,
    "curriculum_warmup_steps": 2000,
    "curriculum_total_steps": 10000,
    "num_workers": 2,
    "prefetch_batches": 2,
    "shuffle_buffer_size": 5000
  },
  "training": {
    "learning_rate": 0.0003,
    "min_learning_rate": 0.00001,
    "weight_decay": 0.01,
    "beta1": 0.9,
    "beta2": 0.95,
    "max_grad_norm": 1.0,
    "warmup_steps": 500,
    "max_steps": 10000,
    "scheduler": "cosine",
    "wsd_stable_ratio": 0.8,
    "wsd_decay_ratio": 0.1,
    "batch_size": 16,
    "gradient_accumulation_steps": 4,
    "use_amp": true,
    "amp_dtype": "float16",
    "checkpoint_dir": "./checkpoints/tiny-mlx",
    "save_every_n_steps": 1000,
    "keep_last_n_checkpoints": 3,
    "log_every_n_steps": 50,
    "eval_every_n_steps": 500,
    "gradient_checkpointing": true
  },
  "sft": {
    "use_lora": true,
    "lora_r": 8,
    "lora_alpha": 16,
    "lora_dropout": 0.05,
    "lora_target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj"
    ],
    "use_neftune": true,
    "neftune_alpha": 5.0,
    "learning_rate": 0.00002,
    "num_epochs": 3,
    "warmup_ratio": 0.03
  },
  "grpo": {
    "beta": 0.01,
    "group_size": 4,
    "reward_model_path": null,
    "use_rule_based_reward": true,
    "learning_rate": 0.000001,
    "num_iterations": 1000,
    "kl_target": 0.1
  },
  "distillation": {
    "teacher_model_path": "",
    "temperature": 4.0,
    "alpha": 0.5,
    "loss_type": "kl",
    "use_hidden_distillation": false,
    "hidden_weight": 0.1,
    "use_progressive": false,
    "num_stages": 3,
    "intermediate_sizes": [
      4096,
      2048,
      1024
    ]
  },
  "export": {
    "output_dir": "./exports",
    "export_safetensors": true,
    "export_gguf": false,
    "export_coreml": false,
    "export_onnx": false,
    "quantize": false,
    "quantization_bits": 8
  },
  "distributed": {
    "num_workers": 1,
    "use_gpu": true,
    "gpus_per_worker": 1,
    "cpus_per_worker": 4,
    "data_parallel_size": 1,
    "tensor_parallel_size": 1,
    "pipeline_parallel_size": 1,
    "expert_parallel_size": 1,
    "backend": "nccl",
    "ray_address": null,
    "runtime_env": {}
  },
  "use_wandb": false,
  "wandb_project": "deepseek-training",
  "wandb_entity": null,
  "output_dir": "./outputs"
}
