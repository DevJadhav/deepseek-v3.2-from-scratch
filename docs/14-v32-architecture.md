# DeepSeek-V3.2 Architecture Summary

This document provides a comprehensive overview of the DeepSeek-V3.2 architecture as implemented in this repository across Rust (Candle), Python (PyTorch/MPS/CUDA), and Python (MLX).

---

## ğŸ¯ Overview

DeepSeek-V3.2 represents the state-of-the-art in large language model architecture, combining:

- **DeepSeek Sparse Attention (DSA)**: Near-linear attention complexity for 128K+ context
- **256-Expert MoE with Hierarchical Routing**: Massive capacity with sparse activation
- **Multi-Token Prediction (MTP)**: Faster training and speculative decoding
- **FP8 Mixed-Precision**: Efficient inference on modern hardware
- **Agent/Tool-Use Training**: Structured environment for training tool-using agents

**Key Papers:**
- [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434) (DeepSeek-AI, 2024)
- [DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437) (DeepSeek-AI, 2024)
- [DeepSeekMoE: Towards Ultimate Expert Specialization](https://arxiv.org/abs/2401.06066) (DeepSeek-AI, 2024)
- [DeepSeekMath: Pushing the Limits of Mathematical Reasoning](https://arxiv.org/abs/2402.03300) (DeepSeek-AI, 2024)

---

## ğŸ—ï¸ Core Components

### 1. DeepSeek Sparse Attention (DSA)

**Complexity**: O(k Ã— L) instead of O(LÂ²)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DSA Attention Pattern                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Query Position â†’                                           â”‚
â”‚  â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”          â”‚
â”‚  â”‚ â–ˆ â”‚ â–ˆ â”‚ â–ˆ â”‚ â–ˆ â”‚ â–‘ â”‚ â–‘ â”‚ â–‘ â”‚ â–ˆ â”‚ â–‘ â”‚ â–‘ â”‚ â–‘ â”‚ â–ˆ â”‚ â† KV     â”‚
â”‚  â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜          â”‚
â”‚  â”œâ”€â”€ Local Window â”€â”€â”¤     â”œâ”€â”€ Global Sampled â”€â”€â”¤            â”‚
â”‚                                                             â”‚
â”‚  â–ˆ = Attended positions (~2K per query)                     â”‚
â”‚  â–‘ = Skipped positions                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Parameters**:
- `window_size`: 4096 (local attention window)
- `num_global_tokens`: 512-1024 (sampled global tokens)
- `dilation_stride`: 128-256 (stride for global sampling)
- `max_seq_len`: 131072 (128K context)

**Implementation Files**:
- Rust: `src/model/sparse_attention.rs`
- PyTorch: `src/deepseek/model/sparse_attention.py`
- MLX: `mlx_impl/attention.py`

---

### 2. Extended RoPE for Long Context

Support for 128K+ context through advanced position encoding scaling.

**Scaling Types**:

| Type | Use Case | Key Parameter |
|------|----------|---------------|
| None | â‰¤4K context | - |
| Linear | Simple extension | `scale: 4.0` |
| NTK-Aware | Better long-range | `alpha: 32.0` |
| YaRN | Best quality | `original_max_seq_len: 4096` |
| Dynamic NTK | Adaptive | `max_position_embeddings` |

**YaRN Interpolation Formula**:

$$\theta_i = \begin{cases}
\theta_i^{original} & \text{if } \lambda_i < \lambda_{fast} \\
\theta_i^{scaled} & \text{if } \lambda_i > \lambda_{slow} \\
(1-\gamma)\theta_i^{original} + \gamma\theta_i^{scaled} & \text{otherwise}
\end{cases}$$

**Implementation Files**:
- Rust: `src/model/mla.rs` (ExtendedRotaryPositionalEncoding)
- PyTorch: `src/deepseek/model/mla.py`
- MLX: `mlx_impl/attention.py`

---

### 3. MoE V3 with 256 Experts

**Architecture**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     MoE V3 Architecture                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Input â”€â”€â”¬â”€â”€â–º Shared Experts (2) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚          â”‚                                         â”‚         â”‚
â”‚          â””â”€â”€â–º Router â”€â”€â–º Hierarchical Selection    â”‚         â”‚
â”‚                    â”‚                               â”‚         â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”                        â”‚         â”‚
â”‚           â–¼               â–¼                        â”‚         â”‚
â”‚     Group Selection  Expert Selection              â”‚         â”‚
â”‚     (top-4 of 8)     (top-2 per group)             â”‚         â”‚
â”‚           â”‚               â”‚                        â”‚         â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚         â”‚
â”‚                   â–¼                                â”‚         â”‚
â”‚           Selected Experts (8 of 256)              â”‚         â”‚
â”‚                   â”‚                                â”‚         â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â–º Combine â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                â”‚                             â”‚
â”‚                                â–¼                             â”‚
â”‚                             Output                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Features**:
- **256 routed experts** with 8 active per token
- **2 shared experts** always active
- **Hierarchical routing**: Groups â†’ Experts within groups
- **Auxiliary-loss-free load balancing**: Bias-based adjustment
- **Expert capacity**: 1.25x factor with dropout handling

**Configuration**:
```rust
DeepSeekMoEV3Config {
    d_model: 4096,
    n_routed_experts: 256,
    n_shared_experts: 2,
    top_k: 8,
    n_expert_groups: 8,      // 256 / 8 = 32 experts per group
    top_k_groups: 4,         // Select 4 groups, then 2 per group
    capacity_factor: 1.25,
    aux_loss_free: true,
}
```

**Implementation Files**:
- Rust: `src/model/moe.rs`
- PyTorch: `src/deepseek/model/moe.py`
- MLX: `mlx_impl/moe.py`

---

### 4. Multi-Token Prediction (MTP)

Enables speculative decoding for 2-4x faster inference.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MTP Architecture                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  Base Model â”€â”€â–º Hidden States â”€â”€â–º LM Head â”€â”€â–º Token t+1    â”‚
â”‚                      â”‚                                     â”‚
â”‚                      â”œâ”€â”€â–º MTP[0] â”€â”€â–º Token t+2             â”‚
â”‚                      â”‚                                     â”‚
â”‚                      â”œâ”€â”€â–º MTP[1] â”€â”€â–º Token t+3             â”‚
â”‚                      â”‚                                     â”‚
â”‚                      â””â”€â”€â–º MTP[2] â”€â”€â–º Token t+4             â”‚
â”‚                                                            â”‚
â”‚  Speculative Decoding:                                     â”‚
â”‚  1. Draft k tokens using MTP heads                         â”‚
â”‚  2. Verify all k tokens in parallel with target model      â”‚
â”‚  3. Accept matching prefix, reject remainder               â”‚
â”‚  4. Resample from target at first rejection                â”‚
â”‚                                                            â”‚
â”‚  Expected Speedup: 2-4x for high acceptance rates          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation Files**:
- Rust: `src/model/mtp.rs`, `src/model/inference.rs`
- PyTorch: `src/deepseek/model/mtp.py`
- MLX: `mlx_impl/mtp.py`

---

### 5. FP8 Mixed-Precision Training

Tile-based quantization for efficient computation.

**Format**: E4M3 for weights/activations, E5M2 for gradients

**Tile Scaling**:
$$x_{fp8}[i] = \text{round}\left(\frac{x[i]}{s_{tile}}\right)$$

where $s_{tile} = \max_{j \in tile}|x[j]| / \text{FP8\_MAX}$

**Benefits**:
- 2x memory reduction vs FP16
- ~1.5x throughput on H100/M3+
- <0.5% accuracy loss with proper scaling

**Implementation Files**:
- Rust: `src/model/quantization.rs`
- PyTorch: `src/deepseek/model/quantization.py`
- MLX: `mlx_impl/quantization.py`

---

### 6. Agent/Tool-Use Training (Phase 5)

Structured curriculum for training tool-using agents.

**Task Tiers**:

| Tier | Description | Environments | Difficulty |
|------|-------------|--------------|------------|
| Single Tool | One tool, one call | 200 | 1.0 |
| Multi-Tool Sequential | Chain of tools | 400 | 1.5 |
| Multi-Tool Parallel | Concurrent tool use | 600 | 2.0 |
| Complex Workflow | Multi-step reasoning | 600 | 3.0 |

**Reward Components**:
- **Correctness** (50%): Task completion accuracy
- **Format** (20%): Valid tool-call JSON structure
- **Efficiency** (15%): Minimize unnecessary calls
- **Safety** (15%): No harmful operations

**Implementation Files**:
- Rust: `src/training/agent.rs`
- PyTorch: `src/deepseek/training/agent.py`
- MLX: `mlx_impl/agent.py`

---

## ğŸš€ Training Infrastructure

### DualPipe Parallel Strategy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DualPipe Scheduling                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  GPU 0: [F0][F1][F2][F3][B3][B2][B1][B0]                     â”‚
â”‚  GPU 1: [  ][F0][F1][F2][F3][B3][B2][B1]                     â”‚
â”‚  GPU 2: [  ][  ][F0][F1][F2][F3][B3][B2]                     â”‚
â”‚  GPU 3: [  ][  ][  ][F0][F1][F2][F3][B3]                     â”‚
â”‚                                                              â”‚
â”‚  F = Forward pass, B = Backward pass                         â”‚
â”‚  Bidirectional scheduling minimizes bubble time              â”‚
â”‚                                                              â”‚
â”‚  Bubble Ratio: 1/(2p-1) where p = pipeline stages            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ZeRO-3 Memory Optimization

- Partition optimizer states, gradients, and parameters
- All-gather for forward, reduce-scatter for backward
- Offload to CPU/NVMe for extreme cases

**Implementation Files**:
- Rust: `src/distributed/pipeline.rs`
- PyTorch: `src/deepseek/distributed/pipeline.py`

---

## ğŸ“Š Performance Characteristics

### Memory Usage

| Component | Memory (per token, FP16) |
|-----------|--------------------------|
| DSA KV Cache | ~0.1x vs MHA |
| MoE (256 experts, 8 active) | ~3% of total params active |
| MTP Heads (k=4) | +20% over base |

### Compute Scaling

| Model Size | Context | DSA Speedup vs MHA |
|------------|---------|-------------------|
| 7B | 8K | 1.5x |
| 7B | 32K | 3.0x |
| 7B | 128K | 8.0x |

### Speculative Decoding Performance

| Acceptance Rate | Tokens/Round | Speedup |
|-----------------|--------------|---------|
| 90% | 3.6 | 3.6x |
| 70% | 2.1 | 2.1x |
| 50% | 1.5 | 1.5x |

---

## ğŸ§ª Testing

### Rust Tests
```bash
# Run all V3.2 integration tests
cargo test --test v32_integration_tests

# Run specific component tests
cargo test moe
cargo test inference
cargo test agent
```

### Python Tests
```bash
# PyTorch tests
pytest tests/test_pytorch_v32.py -v
pytest tests/test_phase6_production.py -v

# MLX tests
pytest tests/test_mlx_v32.py -v
pytest tests/test_phase6_mlx.py -v
```

---

## ğŸ“ Repository Structure

```
â”œâ”€â”€ Deepseek-from-scratch-in-rust/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”‚   â”œâ”€â”€ sparse_attention.rs  # DSA
â”‚   â”‚   â”‚   â”œâ”€â”€ mla.rs               # Extended RoPE
â”‚   â”‚   â”‚   â”œâ”€â”€ moe.rs               # MoE V3
â”‚   â”‚   â”‚   â”œâ”€â”€ mtp.rs               # Multi-Token Prediction
â”‚   â”‚   â”‚   â”œâ”€â”€ inference.rs         # Speculative Decoding
â”‚   â”‚   â”‚   â””â”€â”€ quantization.rs      # FP8
â”‚   â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”‚   â”œâ”€â”€ agent.rs             # Agent training
â”‚   â”‚   â”‚   â””â”€â”€ grpo.rs              # GRPO alignment
â”‚   â”‚   â””â”€â”€ distributed/
â”‚   â”‚       â””â”€â”€ pipeline.rs          # DualPipe
â”‚   â””â”€â”€ tests/
â”‚       â””â”€â”€ v32_integration_tests.rs
â”‚
â”œâ”€â”€ deepseek-from-scratch-python/
â”‚   â”œâ”€â”€ src/deepseek/                # PyTorch implementation
â”‚   â”œâ”€â”€ mlx_impl/                    # MLX implementation
â”‚   â””â”€â”€ tests/
â”‚       â”œâ”€â”€ test_pytorch_v32.py
â”‚       â”œâ”€â”€ test_mlx_v32.py
â”‚       â”œâ”€â”€ test_phase6_production.py
â”‚       â””â”€â”€ test_phase6_mlx.py
â”‚
â””â”€â”€ docs/                            # Documentation
    â”œâ”€â”€ 01-multi-query-attention.md
    â”œâ”€â”€ ...
    â””â”€â”€ 14-v32-architecture.md       # This document
```

---

## ğŸ“š References

1. DeepSeek-V3 Technical Report (2024)
2. YaRN: Efficient Context Window Extension (2023)
3. Mixtral of Experts (2024)
4. Speculative Decoding (2022)
5. FP8 Training: A Practitioner's Guide (2023)

---

## ğŸ‰ Implementation Status

| Phase | Component | Rust | PyTorch | MLX |
|-------|-----------|------|---------|-----|
| 1 | DSA | âœ… | âœ… | âœ… |
| 1 | Extended RoPE | âœ… | âœ… | âœ… |
| 2 | MoE V3 | âœ… | âœ… | âœ… |
| 3 | DualPipe | âœ… | âœ… | âœ… |
| 3 | ZeRO-3 | âœ… | âœ… | N/A |
| 4 | FP8 | âœ… | âœ… | âœ… |
| 5 | Agent Training | âœ… | âœ… | âœ… |
| 6 | Integration Tests | âœ… | âœ… | âœ… |
| 6 | Speculative Decoding | âœ… | âœ… | âœ… |

All phases complete! ğŸŠ
