# DeepSeek from Scratch

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Rust](https://img.shields.io/badge/rust-stable-orange.svg)](https://www.rust-lang.org/)

Educational implementations of **DeepSeek-V3.2** and **DeepSeek-R1** architectures in **Rust** (using Candle) and **Python** (using PyTorch/MLX).

This repository provides from-scratch implementations of the key innovations that make DeepSeek models state-of-the-art:

### ğŸ§  Attention Mechanisms
- **Multi-Query Attention (MQA)** - Single KV head for memory-efficient inference
- **Grouped-Query Attention (GQA)** - Balanced KV sharing across head groups
- **Multi-Head Latent Attention (MLA)** - Compressed KV cache for efficient inference
- **DeepSeek Sparse Attention (DSA)** - Hybrid local + dilated global attention patterns

### ğŸ”€ Mixture of Experts
- **Standard MoE** - Top-k expert routing with load balancing
- **DeepSeek MoE** - Fine-grained experts with shared expert isolation
- **256-Expert MoE** - Hierarchical routing for massive expert scaling

### ğŸ¯ Prediction & Quantization
- **Multi-Token Prediction (MTP)** - Predict multiple future tokens simultaneously
- **FP8 Mixed-Precision** - Low-precision training with dynamic scaling
- **FP8 Quantization** - Simulated 8-bit inference for deployment

### ğŸ‹ï¸ Training & Alignment
- **GRPO Training** - Group Relative Policy Optimization for RL
- **DPO Training** - Direct Preference Optimization
- **SFT Pipeline** - Supervised Fine-Tuning infrastructure
- **Knowledge Distillation** - Teacher-student model compression
- **Agent & Tool-Use Training** - Function calling and tool integration

### ğŸš€ Infrastructure
- **5D Parallelism** - Tensor, Pipeline, Data, Expert, and Sequence parallelism
- **ZeRO Optimization** - Memory-efficient distributed training
- **DeepSeek-R1 Reasoning** - Chain-of-thought reasoning with `<think>` tags
- **Modal Cloud GPUs** - Distributed training on A100/H100 GPUs

---

## ğŸ“– Table of Contents

- [Quick Start](#-quick-start)
- [Prerequisites](#-prerequisites)
- [Installation](#-installation)
- [Training Guide](#-training-guide)
- [Performance Benchmarks](#-performance-benchmarks)
- [Project Structure](#-project-structure)
- [Architecture Documentation](#-architecture-documentation)
- [Development](#-development)
- [Contributing](#-contributing)
- [License](#-license)
- [References](#-references)

---

## ğŸš€ Quick Start

### Train a Model in 5 Minutes

```bash
# 1. Clone and setup
git clone https://github.com/DevJadhav/deepseek-from-scratch.git
cd DeepSeek-From-Scratch

# 2. Install dependencies
curl -LsSf https://astral.sh/uv/install.sh | sh  # Install UV if needed
uv sync

# 3. Download training data
uv run python scripts/download_tinystories.py

# 4. Train! (Choose one option)

# Option A: Local MLX (Apple Silicon - fastest for local dev)
uv run python -m ray_pipeline.cli run-mlx --max-steps 1000

# Option B: Modal Cloud GPU (Recommended for production)
pip install modal && modal setup
python -m ray_pipeline.cli run-rust --gpus 3 --pp-size 3 --max-steps 3000

# Option C: Local PyTorch (CPU/CUDA)
uv run python -m ray_pipeline.cli run --backend pytorch --model-size tiny --max-steps 1000
```

### Run Demos & Benchmarks

```bash
# PyTorch demos (CUDA/MPS/CPU)
cd deepseek-from-scratch-python
uv run python src/deepseek/main.py

# MLX demos (Apple Silicon native)
uv run python mlx_impl/main.py
uv run python mlx_impl/benchmark.py

# Rust demos (Metal)
cd Deepseek-from-scratch-in-rust
cargo run --release
```

---

## ğŸ› ï¸ Prerequisites

### System Requirements

- **macOS 12.3+** (for Metal/MPS) or **Linux with CUDA**
- **Apple Silicon (M1/M2/M3/M4)** recommended for best local performance
- **8GB+ RAM** recommended (16GB+ for larger models)

### Required Tools

| Tool | Purpose | Installation |
|------|---------|--------------|
| **Python 3.10+** | Python implementation | [python.org](https://www.python.org/downloads/) |
| **UV** | Fast Python package manager | `curl -LsSf https://astral.sh/uv/install.sh \| sh` |
| **Rust** | Rust implementation | [rustup.rs](https://rustup.rs/) |
| **Modal** (optional) | Cloud GPU training | `pip install modal && modal setup` |

---

## ğŸ“¦ Installation

### Python Setup (Recommended)

```bash
cd DeepSeek-From-Scratch

# Install with UV (fastest)
uv sync

# Or install with all optional extras
uv sync --all-extras  # Includes MLX, CoreML, dev tools
```

**Alternative (pip):**
```bash
pip install torch numpy einops transformers
pip install mlx  # Optional: Apple Silicon only
pip install coremltools  # Optional: CoreML export
```

### Rust Setup

```bash
cd Deepseek-from-scratch-in-rust

# Build in release mode (required for Metal acceleration)
cargo build --release
```

---

## ğŸ“ Training Guide

### Training Data Setup

```bash
# Download TinyStories dataset
uv run python scripts/download_tinystories.py
# Data saved to: data/stories/
```

### Training Options

#### Option 1: Modal Cloud GPUs (Production Recommended)

Best for: Production training, large-scale experiments

```bash
# Setup Modal (one-time)
pip install modal
modal setup

# Rust backend (fastest - 13.5 steps/sec)
python -m ray_pipeline.cli run-rust --gpus 3 --pp-size 3 --max-steps 3000

# Python backend (more flexible - 10.2 steps/sec)
python -m ray_pipeline.cli run-python --gpus 3 --pp-size 3 --max-steps 3000

# Full time-sliced execution (alternates Rust/Python)
python -m ray_pipeline.cli run --time-sliced --gpus 3 --pp-size 3 --max-steps 3000
```

#### Option 2: Local MLX (Apple Silicon)

Best for: Local development, quick iterations on Mac

```bash
# Memory-conscious config
uv run python -m ray_pipeline.cli run-mlx --max-steps 1500 --batch-size 2 --d-model 128

# Full config
uv run python -m ray_pipeline.cli run --backend mlx --model-size tiny --max-steps 5000
```

#### Option 3: Local PyTorch (CPU/CUDA)

Best for: Linux with CUDA, debugging

```bash
uv run python -m ray_pipeline.cli run --backend pytorch --model-size tiny --max-steps 1000
```

### Training Pipeline Stages

The ray_pipeline orchestrates a complete training workflow:

```
DATA_PREP â†’ PRETRAIN â†’ SFT â†’ GRPO â†’ DISTILLATION â†’ EXPORT
```

| Stage | Description |
|-------|-------------|
| **DATA_PREP** | Tokenize and shard dataset |
| **PRETRAIN** | MTP + MoE pretraining |
| **SFT** | Supervised Fine-Tuning (instruction tuning) |
| **GRPO** | Group Relative Policy Optimization (alignment) |
| **DISTILLATION** | Knowledge distillation (optional) |
| **EXPORT** | Save final model + config |

### 5D Parallelism Configuration

The framework implements DeepSeek-style 5D parallelism:

| Dimension | Description | Default |
|-----------|-------------|---------|
| **PP** (Pipeline) | Splits model layers across GPUs | 3 |
| **DP** (Data) | Replicates model, splits data | 1 |
| **TP** (Tensor) | Splits layers horizontally | 1 |
| **EP** (Expert) | Distributes MoE experts | 1 |
| **SP** (Sequence) | Splits long sequences | 1 |

**Pipeline Parallelism Architecture (PP=3):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    GPU 0     â”‚â”€â”€â–¶â”‚    GPU 1     â”‚â”€â”€â–¶â”‚    GPU 2     â”‚
â”‚ Embed+L1-4   â”‚   â”‚   L5-8       â”‚   â”‚ L9-12+Head   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–²                                     â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Gradient Flow â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Model Export

```bash
# Export to GGUF format
uv run python scripts/export_gguf.py --checkpoint checkpoints/final

# Export to CoreML (iOS/macOS)
uv run python deepseek-from-scratch-python/export_coreml.py
```

### Run Inference

```bash
uv run python scripts/inference.py --checkpoint checkpoints/final --prompt "Once upon a time"
```

---

## ğŸ“Š Performance Benchmarks

### Training Benchmarks (3000 steps)

| Backend | Hardware | Time | Steps/sec | Final Loss |
|---------|----------|------|-----------|------------|
| **Rust+GPU** | 3Ã— H100 80GB | ~4 min | **13.5** | **1.18** |
| **Python+GPU** | 3Ã— H100 80GB | ~5 min | 10.2 | 1.37 |
| **MLX** | Apple M1/M2/M3 | ~15 min | 3.3 | 1.85 |

### Component Benchmarks (Apple Silicon)

**Test Config:** batch_size=4, seq_len=64, d_model=512

#### Attention Mechanisms

| Component | Rust (Metal) | Python (MPS) | MLX |
|-----------|-------------|--------------|-----|
| **MQA** | 11.75ms | 0.95ms | 0.73ms |
| **GQA** | 11.00ms | 0.54ms | 0.82ms |
| **MLA** | 10.74ms | 0.96ms | 0.97ms |

#### Mixture of Experts

| Component | Rust (Metal) | Python (MPS) | MLX |
|-----------|-------------|--------------|-----|
| **Standard MoE** | 5.94ms | 134.87ms | - |
| **DeepSeek MoE** | 4.97ms | 49.85ms | 2.53ms |

#### Training Operations

| Component | Rust (Metal) | Python (MPS) | MLX |
|-----------|-------------|--------------|-----|
| **GRPO Loss** | 0.04ms | 0.73ms | 0.66ms |
| **DPO Loss** | 0.01ms | 0.28ms | 1.08ms |
| **KD Loss** | 0.05ms | 0.61ms | 0.32ms |

### Running Benchmarks

```bash
# PyTorch benchmarks (CUDA/MPS/CPU)
cd deepseek-from-scratch-python
uv run python -m pytest tests/ -v

# MLX benchmarks (Apple Silicon native)
uv run python mlx_impl/benchmark.py

# Rust benchmarks (Metal)
cd Deepseek-from-scratch-in-rust
cargo run --release
```

---

## ğŸ“ Project Structure

```
DeepSeek-From-Scratch/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ LICENSE                      # Apache 2.0 License
â”œâ”€â”€ pyproject.toml               # Python dependencies
â”œâ”€â”€ uv.lock                      # Locked dependencies
â”‚
â”œâ”€â”€ deepseek-from-scratch-python/
â”‚   â”œâ”€â”€ src/deepseek/            # PyTorch implementation (CUDA/MPS/CPU)
â”‚   â”‚   â”œâ”€â”€ main.py              # Entry point
â”‚   â”‚   â”œâ”€â”€ model/               # Model components
â”‚   â”‚   â”‚   â”œâ”€â”€ attention.py     # MQA, GQA
â”‚   â”‚   â”‚   â”œâ”€â”€ mla.py           # MLA, DeepSeek Attention
â”‚   â”‚   â”‚   â”œâ”€â”€ moe.py           # MoE implementations
â”‚   â”‚   â”‚   â”œâ”€â”€ mtp.py           # Multi-Token Prediction
â”‚   â”‚   â”‚   â”œâ”€â”€ transformer.py   # Full transformer model
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â””â”€â”€ training/            # Training infrastructure
â”‚   â”‚
â”‚   â”œâ”€â”€ mlx_impl/                # MLX implementation (Apple Silicon native)
â”‚   â”‚   â”œâ”€â”€ main.py              # Entry point
â”‚   â”‚   â”œâ”€â”€ benchmark.py         # Benchmarks
â”‚   â”‚   â”œâ”€â”€ attention.py         # MQA, GQA, MLA
â”‚   â”‚   â”œâ”€â”€ moe.py               # MoE implementations
â”‚   â”‚   â”œâ”€â”€ mtp.py               # Multi-Token Prediction
â”‚   â”‚   â”œâ”€â”€ grpo.py              # GRPO training
â”‚   â”‚   â”œâ”€â”€ r1.py                # DeepSeek-R1 reasoning
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ tests/                   # Test suite
â”‚
â”œâ”€â”€ Deepseek-from-scratch-in-rust/  # Rust/Candle implementation (Metal)
â”‚   â”œâ”€â”€ Cargo.toml               # Rust dependencies
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ main.rs              # Entry point
â”‚       â”œâ”€â”€ model/               # Model components
â”‚       â””â”€â”€ training/            # Training infrastructure
â”‚
â”œâ”€â”€ ray_pipeline/                # Training orchestration
â”‚   â”œâ”€â”€ cli.py                   # Command-line interface
â”‚   â”œâ”€â”€ config.py                # Configuration
â”‚   â”œâ”€â”€ workflow.py              # Ray Workflow DAG
â”‚   â”œâ”€â”€ stages/                  # Pipeline stages
â”‚   â””â”€â”€ runners/                 # Backend runners
â”‚
â”œâ”€â”€ modal_gpu/                   # Modal cloud GPU integration
â”‚   â”œâ”€â”€ app.py                   # Modal app definition
â”‚   â”œâ”€â”€ config.py                # 5D parallelism config
â”‚   â””â”€â”€ distributed_trainer.py   # GPU training runner
â”‚
â”œâ”€â”€ docs/                        # Architecture documentation (22 files)
â”‚
â”œâ”€â”€ scripts/                     # Utility scripts
â”‚   â”œâ”€â”€ download_tinystories.py  # Download training data
â”‚   â”œâ”€â”€ export_gguf.py           # GGUF export
â”‚   â”œâ”€â”€ inference.py             # Run inference
â”‚   â””â”€â”€ train_tiny.py            # Quick training script
â”‚
â””â”€â”€ configs/                     # Configuration files
```

---

## ğŸ“š Architecture Documentation

The `docs/` directory contains in-depth explanations of all architectural components:

### Attention Mechanisms
- [Multi-Query Attention (MQA)](docs/01-multi-query-attention.md)
- [Grouped-Query Attention (GQA)](docs/02-grouped-query-attention.md)
- [Multi-Head Latent Attention (MLA)](docs/03-multi-head-latent-attention.md)
- [DeepSeek Attention](docs/04-deepseek-attention.md)

### Mixture of Experts
- [Standard MoE](docs/05-standard-moe.md)
- [DeepSeek MoE](docs/06-deepseek-moe.md)

### Prediction & Quantization
- [Multi-Token Prediction (MTP)](docs/07-multi-token-prediction.md)
- [FP8 Quantization](docs/08-fp8-quantization.md)

### Training & Alignment
- [GRPO](docs/09-grpo.md)
- [Training Infrastructure](docs/10-training-infrastructure.md)
- [Training Pipeline](docs/11-training-pipeline.md)
- [Post-Training: SFT & RLHF](docs/12-post-training.md)
- [Knowledge Distillation](docs/13-knowledge-distillation.md)

### Advanced Topics
- [V3.2 Architecture Summary](docs/14-v32-architecture.md)
- [5D Parallelism](docs/15-5d-parallelism.md)
- [ZeRO Optimization](docs/16-zero-optimization.md)
- [Sparse Attention](docs/17-deepseek-sparse-attention.md)

---

## ğŸ”§ Development

### Running Tests

```bash
# Python tests
cd deepseek-from-scratch-python
uv run pytest

# Rust tests
cd Deepseek-from-scratch-in-rust
cargo test
```

### Code Formatting

```bash
# Python
uv run black .
uv run ruff check .

# Rust
cargo fmt
cargo clippy
```

### Type Checking

```bash
# Python
uv run mypy ray_pipeline/
```

---

## ğŸ¤ Contributing

Contributions are welcome! Here are some areas of interest:

- Flash Attention integration
- KV-Cache implementation
- Real FP8 hardware kernels
- Distributed training improvements
- Model weight loading from HuggingFace
- Additional cloud GPU providers (RunPod, Lambda Labs)
- Documentation improvements

### How to Contribute

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

---

## ğŸ“š References

- [DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437)
- [DeepSeek-R1 Technical Report](https://arxiv.org/abs/2501.12948)
- [Candle ML Framework](https://github.com/huggingface/candle)
- [MLX Framework](https://github.com/ml-explore/mlx)
- [Modal Cloud Platform](https://modal.com/)
- [Ray Framework](https://www.ray.io/)

---

## â­ Acknowledgments

This project is for educational purposes, demonstrating the key architectural innovations in DeepSeek models. Special thanks to:

- DeepSeek AI for their open research and technical reports
- Hugging Face for the Candle framework
- Apple for the MLX framework
- The open-source ML community
